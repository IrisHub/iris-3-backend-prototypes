{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # Get bylines from Wenxue City articles\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "## THESE ARE NOT NEEDED\n",
    "# %matplotlib inline\n",
    "# import csv\n",
    "# import matplotlib.dates as dates\n",
    "# import datetime\n",
    "# import time\n",
    "# plt.rcParams['font.sans-serif']=['SimHei'] \n",
    "# plt.rcParams['axes.unicode_minus']=False\n",
    "\n",
    "import bs4\n",
    "import requests\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "url = 'https://www.wenxuecity.com/' # 文学城 activity page\n",
    "\n",
    "\n",
    "# ## HOMEPAGE (wenxuecheng.com) \n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "# Scrape URLs from homepage\n",
    "page_content = requests.get(\"https://www.wenxuecity.com/\", verify=False).content\n",
    "soup = bs4.BeautifulSoup(page_content, 'lxml')\n",
    "results = soup.select(\"div div ul li\")\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "# Grabs URL and headline from each soup object\n",
    "headlines = []\n",
    "for result in results:\n",
    "    headline_url = result.find('a').attrs['href']\n",
    "    headline = result.find('a').contents[0]\n",
    "    headline = headline.replace(\"\\r\\n\", \"\").strip()\n",
    "    headlines.append((headline, headline_url))\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "headlines\n",
    "\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "\n",
    "def get_author(url):\n",
    "    page_content = requests.get(url, verify=False).content # import without checking SSL\n",
    "    soup = bs4.BeautifulSoup(page_content, 'lxml') # parser that works with Chinese characters bs4\n",
    "    author = soup.find('span', itemprop=\"author\") # Grabs author\n",
    "    return author\n",
    "\n",
    "\n",
    "# In[54]:\n",
    "\n",
    "\n",
    "authors = [] # Tuples in format (\"This is a headline\", \"/this/is/a/URL.html\")\n",
    "for headline in headlines:\n",
    "    if headline[1].find('.com') == -1: # If not an ad/external link\n",
    "        url = \"https://www.wenxuecity.com/\" + headline[1]  \n",
    "        author = get_author(url)\n",
    "        if author != None:\n",
    "            authors.append(author.text)\n",
    "\n",
    "\n",
    "# In[55]:\n",
    "\n",
    "\n",
    "df_author = pd.DataFrame(authors)\n",
    "\n",
    "\n",
    "# In[58]:\n",
    "\n",
    "\n",
    "df_author[0].value_counts()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # News stories only -- archived stories\n",
    "\n",
    "# To get more articles, change the \"i\" in \"scrape_headlines()\". Right now set to 10, but can go back to 500+. \n",
    "\n",
    "# In[90]:\n",
    "\n",
    "\n",
    "# Same as above\n",
    "def get_author(url):\n",
    "    page_content = requests.get(url, verify=False).content # import without checking SSL\n",
    "    soup = bs4.BeautifulSoup(page_content, 'lxml') # parser that works with Chinese characters bs4\n",
    "    author = soup.find('span', itemprop=\"author\") # Grabs author\n",
    "    return author\n",
    "\n",
    "\n",
    "# In[78]:\n",
    "\n",
    "\n",
    "# Scrape each page in range(i, 0 to max)\n",
    "def scrape_headlines():\n",
    "    headlines_soup = []\n",
    "    for i in range(10):      ## CHANGE THIS i FOR MORE PAGES\n",
    "        page_content3 = requests.get(\"https://www.wenxuecity.com/news/morenews/?page=\" + str(i), verify=False).content\n",
    "        soup3 = bs4.BeautifulSoup(page_content3, 'lxml')\n",
    "        results3 = soup3.select(\"div div ul li\")\n",
    "        headlines_soup.append(results3)\n",
    "    return headlines_soup\n",
    "\n",
    "\n",
    "# In[80]:\n",
    "\n",
    "\n",
    "# With a big i, this can take a very long time (my machine is ~20 min for i=10)\n",
    "soup_headlines = scrape_headlines()\n",
    "\n",
    "\n",
    "# In[87]:\n",
    "\n",
    "\n",
    "def get_headlines(soup_headlines):\n",
    "    headlines_and_url = []\n",
    "    for soup_object in soup_headlines: # Take list of soup objects, iterate through them\n",
    "        for result in soup_object: # For each object\n",
    "            if result.find('a') != None:\n",
    "                headline_url = result.find('a').attrs['href']\n",
    "                headline = result.find('a').contents[0]\n",
    "                headlines_and_url.append((headline, headline_url))\n",
    "    return headlines_and_url\n",
    "\n",
    "\n",
    "# In[91]:\n",
    "\n",
    "\n",
    "authors = []\n",
    "for headline in headlines_and_url:\n",
    "    if headline[1].find('.com') == -1:\n",
    "        url = \"https://www.wenxuecity.com/\" + headline[1]\n",
    "        author = get_author(url)\n",
    "        if author != None:\n",
    "            authors.append(author.text)\n",
    "\n",
    "\n",
    "# In[93]:\n",
    "\n",
    "\n",
    "df_author2 = pd.DataFrame(authors)\n",
    "\n",
    "\n",
    "# In[102]:\n",
    "\n",
    "\n",
    "df_author2[0].value_counts().head(30)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
